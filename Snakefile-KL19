# pipeline to infer dispersal rates and locate genetic ancestors with spacetrees (Osmond & Coop 2024)

# paths 
datadir = 'data/' #relative path to data directory
spdir = 'KL19_macrocystis/' #species specific folder in datadir
indir = 'output_byChr_byPop_params01/' #and relate output subfolder within that
outdir = 'output_spacetrees/' #where to put our output within datadir
relatedir = 'relate/' #path to your version of relate

# input data
metadata = datadir + spdir + 'geographicCoords_KL19_macrocystis.txt' 
prefix = 'scaffold_{CHR}_popsize_params01'
ancgz = datadir + spdir + indir + prefix + '.anc.gz' #name of anc files
mutgz = datadir + spdir + indir + prefix + '.mut.gz' #name of mut files
coal = datadir + spdir + indir + prefix + '.coal' #name of coal file

# parameters
CHRS = [str(i) for i in range(1,35) if i!=2] #names of chromosomes you have anc/mut files for
m = '8.135e-10' #estimated mutation rate
Ms = [100] #number of importance samples at each locus
trees_per_chr = [5028, 4654, 4275, 4897, 2637, 4017, 1937, 3420, 3030, 2131, 2510,
       3571, 2710, 1090, 2632, 3396, 1732, 2010,  981, 2091, 2265, 2846,
              3341, 2158, 3115, 3288, 2635, 1012, 2135, 1471,  992, 1860, 1180] #calculated in plots.ipynb
treeskip = 100 #number of trees to skip along genome (thinning)
sampled_trees_per_chr = [len(range(0,i,treeskip)) for i in trees_per_chr] #number of sampled trees per chr
CHRS_rep = []
sampled_trees_chr = []
for i in range(len(CHRS)):
  CHRS_rep += [CHRS[i]] * sampled_trees_per_chr[i] #chr repeated for each sampled tree, to make tuples for expand's zip
  sampled_trees_chr += [treeskip*i for i in range(sampled_trees_per_chr[i])] #sampled trees in each chr
ancestor_times = [10, 100, 1000, 2500, 5000, 10000, 20000, 40000, 100000, 1000000] #generations in the past to locate ancestors at
Ts = [None] #none until time cutoff error fixed

# subsamples
set03 = []
with open (metadata,'r') as f:
  next(f)
  for line in f:
    for _ in range(2):
      set03.append(int(line.split()[-1]))

# ---------------- unzip all anc/mut files ------------------------------

anc = datadir + spdir + indir + prefix + '.anc'
mut = datadir + spdir + indir + prefix + '.mut'

rule gunzip_ancmut:
  input:
    ancgz=ancgz, 
    mutgz=mutgz
  output:
    anc=anc,
    mut=mut
  threads: 1
  resources:
    runtime=15
  shell:
    '''
    gunzip {input.ancgz} 
    gunzip {input.mutgz} 
    '''

# ---------------- get locations ------------------------

locations = datadir + spdir + outdir + 'locations.txt'

rule locations:
  input:
    metadata=metadata
  output:
    locations=locations
  threads: 1
  resources:
    runtime=15
  run:
    with open (input.metadata,'r') as f:
      with open (output.locations,'w') as g:
        next(f)
        for line in f:
          for _ in range(2):
            g.write(' '.join(line.split()[3:5] + ['\n']))

# ---------------- get positions of all loci ------------------------------

loci = datadir + spdir + outdir + prefix + '.loci' #filename for list of loci positions, just changing the suffix from 'anc' to 'loci'

rule loci_positions:
  input:
    mut=mut
  output:
    loci=loci
  threads: 1
  resources:
    runtime=15 #will be much shorter, but my server has 15m minumum 
  run:
    from utils import loci_positions
    loci_positions(input.mut, output.loci)

# the output is a space delimited file with the position of the first and last mutation at each locus, with each locus in a separate row

# ---------------- sample trees at a locus ------------------------------

# now we sample trees at a given locus
# more specifically, Relate fixes the topology at each locus and resamples the branch lengths
# see https://myersgroup.github.io/relate/modules.html#ReEstimateBranchLengths

newick = loci.replace('.loci','_{locus}locus_{M}M.newick')

rule sample_trees:
  input:
    loci=loci,
    anc=anc,
    mut=mut,
    coal=coal
  output:
    newick 
  params:
    prefix_in = anc.replace('.anc',''), #prefix of anc and mut files (relate searches for anc/mut files with this prefix)
    prefix_out = newick.replace('.newick','') #prefix of outfile (relate adds its own suffix)
  threads: 1
  resources:
    runtime=15
  shell:
    '''
    start=$( awk 'NR=={wildcards.locus} + 1 {{print $1}}' {input.loci} ) #position of first snp at locus
    stop=$( awk 'NR=={wildcards.locus} + 1 {{print $2}}' {input.loci} ) #position of last snp at locus
    {relatedir}/scripts/SampleBranchLengths/SampleBranchLengths.sh \
                 -i {params.prefix_in} \
                 --coal {input.coal} \
                 -o {params.prefix_out} \
                 -m {m} \
                 --format n \
                 --num_samples {wildcards.M} \
                 --first_bp $start \
                 --last_bp $stop \
                 --seed 1 
    '''

# ---------------- extract times from trees -----------------------------

# now we will extract the information we need from the trees, the shared times between each pair of lineages and the coalescence times

shared_times = newick.replace('.newick','{subsample}.stss')
coal_times = newick.replace('.newick','{subsample}.ctss') #not actually subsampling because not using below

rule extract_times:
  input:
    newick=newick 
  output:
    stss=shared_times,
    ctss=coal_times
  threads: 1
  resources:
    runtime=15
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # import tools
    import numpy as np
    from tsconvert import from_newick
    from utils import get_shared_times
    from tqdm import tqdm

    # open file of trees to read from
    with open(input.newick, mode='r') as f:
      
      # open files to append to
      with open(output.stss, 'a') as stss:
        with open(output.ctss, 'a') as ctss:

          next(f) #skip header
          for line in tqdm(f, total=int(wildcards.M)): #for each tree sampled
  
            # import tree
            string = line.split()[4] #extract newick string only (Relate adds some info beforehand)
            ts = from_newick(string, min_edge_length=1e-6) #convert to tskit "tree sequence" (only one tree)
            tree = ts.first() #the only tree
  
            # get shared times
            samples = [int(ts.node(node).metadata['name']) for node in ts.samples()] #get index of each sample in list we gave to relate
            sample_order = np.argsort(samples) #get indices to put in ascending order
            ordered_samples = [ts.samples()[i] for i in sample_order] #order samples as in relate
            if wildcards.subsample == '_set03':
              ordered_samples = [j for i,j in enumerate(ordered_samples) if set03[i]==1]
            sts = get_shared_times(tree, ordered_samples) #get shared times between all pairs of samples, with rows and columns ordered as in relate
            stss.write(",".join([str(i) for i in sts]) + '\n') #append as new line
 
            # get coalescence times 
	    ts = ts.simplify(ordered_samples)
	    tree = ts.first()
            cts = sorted([tree.time(i) for i in tree.nodes() if not tree.is_sample(i)]) #coalescence times, in ascending order
            ctss.write(",".join([str(i) for i in cts]) + '\n') #append as new line

# ---------------- process times -----------------------------

# now we process the times, potentially cutting off the tree (to ignore distant past) and getting the exact quantities we need for inference

processed_times = shared_times.replace('.stss','_{T}T{end}')
ends = ['.stss_logdet','_stss_inv.npy','.btss','.lpcs']

rule process_times:
  input:
    stss = shared_times,
    ctss = coal_times,
    coal = coal
  output:
    expand(processed_times, end=ends, allow_missing=True)
  threads: 1 
  resources:
    runtime=15
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # load tools
    import numpy as np
    from utils import chop_shared_times, center_shared_times, log_coal_density
    from tqdm import tqdm

    # determine time cutoff
    T = wildcards.T #get time cutoff
    T = None if T=='None' else float(T) #format correctly

    # effective population size
    epochs = np.genfromtxt(input.coal, skip_header=1, skip_footer=1) #time at which each epoch starts (and the final one ends)
    Nes = 0.5/np.genfromtxt(input.coal, skip_header=2)[2:] #effective population size during each epoch

    # open file of shared times to read from
    with open(input.stss, 'r') as stss:
      with open(input.ctss, 'r') as ctss:

        # open files to write to
        with open(output[0], 'a') as stss_logdet:
          with open(output[2], 'a') as btss:
            with open(output[3], 'a') as lpcs:
          
              # loop over trees at this locus 
              sts_inv = []
              for sts,cts in tqdm(zip(stss,ctss), total=int(wildcards.M)):
          
                # load shared time matrix in vector form
                sts = np.fromstring(sts, dtype=float, sep=',') #convert from string to numpy array
    
                # chop
                sts = chop_shared_times(sts, T=T) #chop shared times to ignore history beyond T
                
                # convert to matrix form
                #k = int((np.sqrt(1+8*(len(sts)-1))+1)/2) #get number of samples (from len(sts) = k(k+1)/2 - k + 1)
                k = int((np.sqrt(1+8*len(sts))-1)/2) #get size of matrix (from sum_i=0^k i = k(k+1)/2), allows for non-contemporary samples
                sts_mat = np.zeros((k,k)) #initialize matrix
                #sts_mat[np.triu_indices(k, k=1)] = sts[1:] #fill in upper triangle
                #sts_mat = sts_mat + sts_mat.T + np.diag([sts[0]]*k) #add lower triangle and diagonal
                sts_mat[np.triu_indices(k, k=0)] = sts #convert to numpy matrix
                sts_mat = sts_mat + sts_mat.T - np.diag(np.diag(sts_mat)) #fill in all entries
                sts = sts_mat
                
                # sample times
                x = np.diag(sts)
                x = np.max(x) - x
                sample_times = np.sort(x)
    
                # center
                sts = center_shared_times(sts) 
          
                # determinant
                sts_logdet = np.linalg.slogdet(sts)[1] #magnitude of log determinant (ignore sign)
                stss_logdet.write(str(sts_logdet) + '\n') #append as new line 
          
                # inverse
                try:
                  stsi = np.linalg.inv(sts) #inverse
                except:
                  stsi = np.linalg.pinv(sts) #pseudo inverse
                stsi = stsi[np.triu_indices(k-1, k=0)] #convert to list
                sts_inv.append(stsi)

                # branching times
                cts = np.fromstring(cts, dtype=float, sep=',') 
                Tmax = cts[-1] #time to most recent common ancestor
                if T is not None and T < Tmax:
                    Tmax = T #farthest time to go back to
                bts = Tmax - np.flip(cts) #branching times, in ascending order
                bts = bts[bts>0] #remove branching times at or before T
                bts = np.append(bts, Tmax) #append total time as last item      
                btss.write(",".join([str(i) for i in bts]) + '\n') #append as new line
               
                # probability of coalescence times under neutral coalescent
                lpc = log_coal_density(coal_times=cts, sample_times=sample_times, Nes=Nes, epochs=epochs, T=Tmax) #log probability density of coalescence times
                lpcs.write(str(lpc) + '\n') #append as new line 

    np.save(output[1],np.array(sts_inv)) #write out as numpy array to avoid numerical issues


# ----------------------- locate ancestors with weightless blup (no dispersal rate or numerical search needed) -----------------------

ancestor_locations_blup = processed_times.replace('{end}','_{s}s_{t}t.blup_locs')

rule locate_ancestors_blup:
  input:
    stss = shared_times,
    stss_inv = processed_times.replace('{end}','_stss_inv.npy'),
    locations = locations
  output:
    ancestor_locations_blup
  threads: 1
  resources:
    runtime=60
  group: "blups"
  run:
    # prevent numpy from using more than {threads} threads (useful for parallizing on my server)
    import os
    os.environ["OMP_NUM_THREADS"] = str(threads)
    os.environ["GOTO_NUM_THREADS"] = str(threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(threads)
    os.environ["MKL_NUM_THREADS"] = str(threads)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(threads)

    # load tools
    import numpy as np
    from tqdm import tqdm
    from spacetrees import locate_ancestors
    from utils import chop_shared_times

    T = wildcards.T #get time cutoff
    T = None if T=='None' else float(T) #format correctly

    # load input data
    # shared times
    stss = np.loadtxt(input.stss, delimiter=',') #list of vectorized shared times matrices
    k = int((np.sqrt(1+8*len(stss[0])-1)+1)/2) #get size of matrix (from sum_i=0^k i = k(k+1)/2)
    mat = np.zeros((k,k))
    mat[np.triu_indices(k, k=0)] = stss[0] #convert to numpy matrix
    mat = mat + mat.T - np.diag(np.diag(mat))      
    x = np.diag(mat) #shared times with self
    sample_times = np.max(x) - x #sampling times
    stss_mat = [] #list of chopped shared times matrices in matrix form
    for sts in stss:
      sts = chop_shared_times(sts, T=T) #chop shared times to ignore history beyond T
      mat = np.zeros((k,k))
      #mat[np.triu_indices(k, k=1)] = sts[1:] #convert to numpy matrix
      #mat = mat + mat.T + np.diag([sts[0]]*k)      
      mat[np.triu_indices(k, k=0)] = sts #convert to numpy matrix
      mat = mat + mat.T - np.diag(np.diag(mat))      
      stss_mat.append(mat)
    stss = stss_mat
    # shared times chopped centered inverted
    stss_inv = np.load(input.stss_inv) #list of vectorized chopped centered inverted shared times matrices
    k = k-1 #get size of matrix
    stss_inv_mat = [] #list of chopped shared times matrices in matrix form
    for sts_inv in stss_inv:
      mat = np.zeros((k,k))
      mat[np.triu_indices(k, k=0)] = sts_inv #convert to numpy matrix
      mat = mat + mat.T - np.diag(np.diag(mat))      
      stss_inv_mat.append(mat)
    stss_inv = stss_inv_mat
    #locations 
    locations = np.loadtxt(input.locations) #location of each sample
    if wildcards.subsample == '_set03':
      locations = np.array([j for i,j in enumerate(locations) if set03[i]==1])

    # locate ancestors
    s = wildcards.s
    if s == 'All': #an option to locate the ancestors of all samples
      samples = range(k+1)   
    else:
      samples = [int(s)]
    t = wildcards.t
    if t == 'All': #an option to locate at pretermined list of times 
      times = ancestor_times
    else: 
      times = [float(t)]
    ancestor_locations = locate_ancestors(samples=samples, times=times, 
                                          shared_times_chopped=stss, shared_times_chopped_centered_inverted=stss_inv, locations=locations, 
                                          sample_times=sample_times, BLUP=True)
    with open(output[0], 'a') as f:
      for anc_loc in ancestor_locations:
        f.write(','.join([str(int(anc_loc[0]))] + [str(i) for i in anc_loc[1:]]) + '\n') #save

# ---------------- dummy rule to run everything you need -----------------

rule all:
  input:
#    expand(anc, CHR=CHRS),
#    locations,
    expand(expand(ancestor_locations_blup, zip, CHR=CHRS_rep, locus=sampled_trees_chr, allow_missing=True), M=Ms, T=Ts, s=['All'], t=['All'], subsample=['','_set03']),

